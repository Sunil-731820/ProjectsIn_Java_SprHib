<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Kogito 1.17.0 released!</title><link rel="alternate" href="https://blog.kie.org/2022/02/kogito-1-17-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/02/kogito-1-17-0-released.html</id><updated>2022-02-21T08:02:01Z</updated><content type="html">We are glad to announce that the Kogito 1.17.0 release is now available! This goes hand in hand with , release. From a feature point of view, we included a series of new features and bug fixes, including: * Data Index support to Oracle database (thanks for the contribution) * Added use cases to examples to see the Management and Task consoles working with PostgreSQL persistence:  and   * Serverless Workflow supports implementation * Serverless Workflow supports implementation * Serverless Workflow “useData” and “useResults” for events and actions. BREAKING CHANGES * Kogito Serverless Workflow Implementation has been upgraded to . Please update your workflow files to align with the updated DSL. For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.16.0 artifacts are available at the . A detailed changelog for 1.17.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title type="html">Getting started with Keycloak powered by Quarkus</title><link rel="alternate" href="http://www.mastertheboss.com/keycloak/getting-started-with-keycloak-powered-by-quarkus/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/keycloak/getting-started-with-keycloak-powered-by-quarkus/</id><updated>2022-02-17T11:34:58Z</updated><content type="html">This article covers the updates in Keycloak which now runs on top of Quarkus. The former (WildFly) distribution of Keycloak is deprecated so you should promptly start the migration process. Keycloak overview Keycloak is an Identity Provider that enables you to secure your Web applications by providing Single Sign-On (SSO) capabilities and leveraging industry standards ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Quality testing the Linux kernel</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/17/quality-testing-linux-kernel" /><author><name>Jeff Bastian</name></author><id>0eddae24-8b03-4454-a4ad-a662012f8390</id><updated>2022-02-17T07:00:00Z</updated><published>2022-02-17T07:00:00Z</published><summary type="html">&lt;p&gt;As a kernel quality engineer at Red Hat, I'm often asked what I do. How does one test a kernel for quality? Testing and debugging the Linux kernel can be challenging because you can't simply attach a debugger like GDB (the GNU Project debugger) if the kernel is crashing: When that happens, everything crashes, including GDB! Even if the kernel isn't crashing, if you ask a debugger to stop the kernel from running so you can inspect something, there is no way to resume running the kernel because the kernel itself is in charge of stopping and starting processes. Asking the kernel to stop itself is a dead-end road.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; There is a way to attach a debugger and stop and resume a kernel, but it requires a second computer and special hardware such as JTAG. Most of the time, I don't have access to that hardware, so that's out of the question.&lt;/p&gt; &lt;p&gt;Many people think that quality testing the Linux kernel must involve writing lots of kernel code; for example, a module to exercise a part of the kernel. Although that is a possibility, most testing actually happens from userspace. I spend a lot of time &lt;em&gt;reading&lt;/em&gt; kernel code to try to devise a test that usually runs as a simple Bash script. There are many ways to interact with the kernel from userspace, including system calls, logs, the &lt;code&gt;procfs&lt;/code&gt; and &lt;code&gt;sysfs&lt;/code&gt; virtual file systems, and more. In this article, I will introduce you to some of the simple tools I've used for investigating behavior in the Linux kernel.&lt;/p&gt; &lt;h2&gt;dmesg and journalctl&lt;/h2&gt; &lt;p&gt;The most basic check on Linux kernel operations, particularly to follow what they're doing with devices and modules, is to retrieve the messages that the kernel generates during its boot and later. The messages can be retrieved from the kernel's logs through a &lt;a href="https://www.linux.org/docs/man1/dmesg.html"&gt;dmesg&lt;/a&gt; or &lt;a href="https://www.linux.org/docs/man1/journalctl.html"&gt;journalctl&lt;/a&gt; command.&lt;/p&gt; &lt;p&gt;For instance, a feature I tested recently involved enabling support for &lt;a href="https://www.kernel.org/doc/html/latest/x86/amd-memory-encryption.html"&gt;AMD Secure Memory Encryption&lt;/a&gt; ( (SME) on AMD's latest CPUs. To enable SME, you have to add &lt;code&gt;mem_encrypt=on&lt;/code&gt; as a kernel parameter and reboot. You can then check for the string "sme" in the &lt;code&gt;dmesg&lt;/code&gt; logs. See the &lt;code&gt;/usr/share/doc/kernel-doc-4.18.0/Documentation/x86/amd-memory-encryption.txt&lt;/code&gt; file from the &lt;code&gt;kernel-doc&lt;/code&gt; RPM in &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 8 for more information.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You may need to enable SME in your BIOS settings, too.&lt;/p&gt; &lt;h3&gt;Enabling SME in dmesg logs&lt;/h3&gt; &lt;p&gt;Let's take a look at the steps to enable SME in dmesg logs.&lt;/p&gt; &lt;p&gt;First, by default, because &lt;code&gt;mem_encrypt&lt;/code&gt; is unset, &lt;code&gt;dmesg&lt;/code&gt; shows no messages:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;~]# dmesg | grep -i sme ~]# &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add the kernel parameter and reboot, and the message is logged:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;~]# grubby --args="mem_encrypt=on" --update-kernel=ALL ~]# reboot ... ~]# dmesg | grep -i sme [ 0.001000] AMD Memory Encryption Features active: SME &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To double-check your work, you can remove the parameter and reboot once more to verify that the message does &lt;em&gt;not&lt;/em&gt; show up in the logs:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;~]# grubby --remove-args="mem_encrypt=on" --update-kernel=ALL ~]# reboot ... ~]# dmesg | grep -i sme ~]# &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Further testing could be required to show that SME is working as expected, but the &lt;code&gt;dmesg&lt;/code&gt; logs give the first evidence that the kernel is properly detecting the hardware feature and enabling support when requested.&lt;/p&gt; &lt;h3&gt;Logging with journalctl&lt;/h3&gt; &lt;p&gt;I mentioned earlier that &lt;code&gt;journalctl&lt;/code&gt; can get the same output as &lt;code&gt;dmesg&lt;/code&gt;. This is because the journal recorded by &lt;code&gt;systemd&lt;/code&gt; includes the normal &lt;code&gt;dmesg&lt;/code&gt; logs along with userspace logs from &lt;code&gt;/var/log/messages&lt;/code&gt; and more, and each entry is annotated with metadata. To extract just the &lt;code&gt;dmesg&lt;/code&gt; equivalent from the journal, enter:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;~]# journalctl --dmesg --output short-monotonic --no-hostname -- Logs begin at Wed 2021-05-26 11:08:19 EDT, end at Wed 2021-05-26 17:01:01 EDT. -- [ 0.000000] kernel: Linux version 4.18.0-305.el8.x86_64 (mockbuild@x86-vm-07.build.eng.bos.redhat.com) (gcc version 8.4.1 20200928 (Red Hat 8.4.1-1) (GCC)) #1 SMP Thu Apr 29 08:54:30 EDT 2021 [ 0.000000] kernel: Command line: BOOT_IMAGE=(hd0,gpt2)/vmlinuz-4.18.0-305.el8.x86_64 root=/dev/mapper/rhel_dell--per7425--02-root ro crashkernel=auto resume=/dev/mapper/rhel_dell--per7425--02-swap rd.lvm.lv=rhel_dell-per7425-02/root rd.lvm.lv=rhel_dell-per7425-02/swap console=ttyS0,115200n81 [ 0.000000] kernel: x86/fpu: Supporting XSAVE feature 0x001: 'x87 floating point registers' [ 0.000000] kernel: x86/fpu: Supporting XSAVE feature 0x002: 'SSE registers' [ 0.000000] kernel: x86/fpu: Supporting XSAVE feature 0x004: 'AVX registers' [ 0.000000] kernel: x86/fpu: xstate_offset[2]: 576, xstate_sizes[2]: 256 [ 0.000000] kernel: x86/fpu: Enabled xstate features 0x7, context size is 832 bytes, using 'compacted' format. [ 0.000000] kernel: BIOS-provided physical RAM map: [ 0.000000] kernel: BIOS-e820: [mem 0x0000000000000000-0x000000000008efff] usable [ 0.000000] kernel: BIOS-e820: [mem 0x000000000008f000-0x000000000008ffff] ACPI NVS ... ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Compare the previous &lt;code&gt;journalctl&lt;/code&gt; output to &lt;code&gt;dmesg&lt;/code&gt; output:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;~]# dmesg [ 0.000000] Linux version 4.18.0-305.el8.x86_64 (mockbuild@x86-vm-07.build.eng.bos.redhat.com) (gcc version 8.4.1 20200928 (Red Hat 8.4.1-1) (GCC)) #1 SMP Thu Apr 29 08:54:30 EDT 2021 [ 0.000000] Command line: BOOT_IMAGE=(hd0,gpt2)/vmlinuz-4.18.0-305.el8.x86_64 root=/dev/mapper/rhel_dell--per7425--02-root ro crashkernel=auto resume=/dev/mapper/rhel_dell--per7425--02-swap rd.lvm.lv=rhel_dell-per7425-02/root rd.lvm.lv=rhel_dell-per7425-02/swap console=ttyS0,115200n81 [ 0.000000] x86/fpu: Supporting XSAVE feature 0x001: 'x87 floating point registers' [ 0.000000] x86/fpu: Supporting XSAVE feature 0x002: 'SSE registers' [ 0.000000] x86/fpu: Supporting XSAVE feature 0x004: 'AVX registers' [ 0.000000] x86/fpu: xstate_offset[2]: 576, xstate_sizes[2]: 256 [ 0.000000] x86/fpu: Enabled xstate features 0x7, context size is 832 bytes, using 'compacted' format. [ 0.000000] BIOS-provided physical RAM map: [ 0.000000] BIOS-e820: [mem 0x0000000000000000-0x000000000008efff] usable [ 0.000000] BIOS-e820: [mem 0x000000000008f000-0x000000000008ffff] ACPI NVS ... ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The two commmands show nearly identical output. The differences are that &lt;code&gt;journalctl&lt;/code&gt; output starts with a &lt;code&gt;-- Logs begin&lt;/code&gt; line and shows the source of each message (the &lt;code&gt;kernel:&lt;/code&gt; prefix before each message).&lt;/p&gt; &lt;h2&gt;sysfs and procfs&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;/sys&lt;/code&gt; directory (&lt;em&gt;aka&lt;/em&gt; &lt;code&gt;sysfs&lt;/code&gt;) and &lt;code&gt;/proc&lt;/code&gt; directory (&lt;em&gt;aka&lt;/em&gt; &lt;code&gt;procfs&lt;/code&gt;) are also important sources of kernel information, and can be used to control the kernel.&lt;/p&gt; &lt;p&gt;As an example, an important feature of some servers is the ability to hot plug CPUs. Before you swap out a CPU, though, you have to tell the kernel to disable that CPU, or the kernel might try to schedule work on a CPU that's in the middle of being swapped.&lt;/p&gt; &lt;p&gt;Let's look at the preparation to hot swap a CPU.&lt;/p&gt; &lt;h3&gt;Disable a CPU for hot swapping&lt;/h3&gt; &lt;p&gt;We start with an &lt;a href="https://www.linux.org/docs/man1/lscpu.html"&gt;&lt;code&gt;lscpu&lt;/code&gt;&lt;/a&gt; command, which lists information about the CPUs, to find what CPUs we have with NUMA support. Then, we disable CPU 11 and check the CPU status again:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;~]# lscpu | grep 'CPU(s)' | grep -v NUMA CPU(s): 96 On-line CPU(s) list: 0-95 ~]# echo 0 &gt; /sys/devices/system/cpu/cpu11/online ~]# lscpu | grep 'CPU(s)' | grep -v NUMA CPU(s): 96 On-line CPU(s) list: 0-10,12-95 Off-line CPU(s) list: 11 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A good way to double-check that the kernel is not trying to use CPU 11 is to verify that no interrupt handlers are assigned to it. To do this check, view the &lt;code&gt;/proc/interrupts&lt;/code&gt; file (the output is truncated here for better visibility):&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;~]# less -SFiX /proc/interrupts CPU0 .... CPU9 CPU10 CPU12 CPU13 C&gt; 0: 169 .... 0 0 0 0 &gt; 4: 0 .... 0 0 0 0 &gt; 8: 0 .... 0 0 0 0 &gt; 9: 0 .... 0 0 0 0 &gt; 10: 0 .... 0 0 0 0 &gt; ... ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the entire column for &lt;code&gt;CPU11&lt;/code&gt; is missing.&lt;/p&gt; &lt;p&gt;After hot-swapping CPU 11 for a new one, turn it back on:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;~]# echo 1 &gt; /sys/devices/system/cpu/cpu11/online ~]# lscpu | grep 'CPU(s)' | grep -v NUMA CPU(s): 96 On-line CPU(s) list: 0-95 ~]# less -SFiX /proc/interrupts CPU0 .... CPU9 CPU10 CPU11 CPU12 C&gt; 0: 169 .... 0 0 0 0 &gt; 4: 0 .... 0 0 0 0 &gt; 8: 0 .... 0 0 0 0 &gt; 9: 0 .... 0 0 0 0 &gt; 10: 0 .... 0 0 0 0 &gt; ... ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;CPU 11 is online and ready to process interrupts again.&lt;/p&gt; &lt;h3&gt;Disable CPU threads and cores&lt;/h3&gt; &lt;p&gt;Of course, modern CPUs have more than one core on each CPU, and each core can have two or more threads, so you'll need to disable all of the threads and cores on a CPU (or "socket") in order to hot swap it. The CPU topology on this particular system is:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;~]# lscpu | grep -e 'Thread(s)' -e 'Core(s)' -e 'Socket(s)' Thread(s) per core: 2 Core(s) per socket: 24 Socket(s): 2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So, to hot swap CPU 11 (that is, one thread of one core), we'll need to disable all of its siblings. This information is also available in &lt;code&gt;sysfs&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;~]# cat /sys/devices/system/cpu/cpu11/topology/core_siblings_list 1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Disable all 48 of these CPUs, and then the system is ready for the physical CPU hot swap.&lt;/p&gt; &lt;h2&gt;crash&lt;/h2&gt; &lt;p&gt;Several years ago, I added support for the &lt;code&gt;RDRAND&lt;/code&gt; assembly instruction to the kernel for Intel's Ivy Bridge line of CPUs. As I mentioned at the start of this article, it's difficult to attach a debugger to the kernel, so I can't just single-step through the assembly code and look for executions of &lt;code&gt;RDRAND&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;I looked at the &lt;a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=628c6246d47b85f5357298601df2444d7f4dd3fd"&gt;upstream commit&lt;/a&gt; that enabled &lt;code&gt;RDRAND&lt;/code&gt; support and noticed a macro named &lt;code&gt;alternative_io&lt;/code&gt; that I had not seen before:&lt;/p&gt; &lt;pre&gt;&lt;code class="c"&gt; alternative_io("movl $0, %0\n\t" \ nop, \ "\n1: " rdrand "\n\t" \ "jc 2f\n\t" \ "decl %0\n\t" \ "jnz 1b\n\t" \ "2:", \ X86_FEATURE_RDRAND, \ ASM_OUTPUT2("=r" (ok), "=a" (*v)), \ "0" (RDRAND_RETRY_LOOPS)); \ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After &lt;a href="https://lwn.net/Articles/164121/"&gt;a bit of research&lt;/a&gt;, I learned that the &lt;code&gt;alternative&lt;/code&gt; set of macros (including variants like the &lt;code&gt;alternative_io&lt;/code&gt; macro shown here) is a mechanism for the kernel to do live patching on itself at boot under certain conditions. As background, a default set of fairly generic assembly instructions is compatible with all x86 CPUs going back to at least the Pentium (and likely back to 80386). But for newer CPUs that support a given hardware feature—&lt;code&gt;X86_FEATURE_RDRAND&lt;/code&gt;, in this case—the kernel rewrites those default instructions with the &lt;code&gt;alternative&lt;/code&gt; code. It does the rewrite on the fly while the kernel is booting.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;alternative&lt;/code&gt; macros are a very cool feature, much more efficient than using an &lt;code&gt;if&lt;/code&gt; statement. A simple approach to this problem, in the absence of &lt;code&gt;alternative&lt;/code&gt; macros, might look like the following (in pseudo-code):&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt; if (CPU does not have feature foo) then generic compatible slow code else fast new code endif &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The problem with this code is that the &lt;code&gt;if&lt;/code&gt; statement has to be evaluated every time this chunk of code is executed, and branch mispredictions are terrible for CPU performance. Modifying the instructions in-place avoids the branches.&lt;/p&gt; &lt;h3&gt;Kernel on disk with RDRAND support&lt;/h3&gt; &lt;p&gt;Let's use &lt;a href="https://man7.org/linux/man-pages/man1/objdump.1.html"&gt;&lt;code&gt;objdump&lt;/code&gt;&lt;/a&gt; to look at the &lt;code&gt;get_random_int&lt;/code&gt; function, along with the alternate instructions section, &lt;code&gt;altinstr_replacement&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="java bash numberLines"&gt;~]# objdump -d /usr/lib/debug/lib/modules/2.6.32-279.el6.x86_64/vmlinux ... ... ffffffff81313780 &lt;get_random_int&gt;: ffffffff81313780: 55 push %rbp ffffffff81313781: 48 89 e5 mov %rsp,%rbp ffffffff81313784: 48 83 ec 20 sub $0x20,%rsp ffffffff81313788: 48 89 5d e8 mov %rbx,-0x18(%rbp) ffffffff8131378c: 4c 89 65 f0 mov %r12,-0x10(%rbp) ffffffff81313790: 4c 89 6d f8 mov %r13,-0x8(%rbp) ffffffff81313794: e8 27 76 cf ff callq ffffffff8100adc0 &lt;mcount&gt; ffffffff81313799: ba 0a 00 00 00 mov $0xa,%edx ffffffff8131379e: ba 00 00 00 00 mov $0x0,%edx ffffffff813137a3: 66 66 66 90 data32 data32 xchg %ax,%ax ffffffff813137a7: 85 d2 test %edx,%edx ffffffff813137a9: 75 50 jne ffffffff813137fb &lt;get_random_int+0x7b&gt; ffffffff813137ab: 65 48 8b 1c 25 b0 e0 mov %gs:0xe0b0,%rbx ... ... ffffffff81d3db6c &lt;.altinstr_replacement&gt;: ... ... ffffffff81d407e7: 48 0f c7 f0 rdrand %rax ffffffff81d407eb: 72 04 jb ffffffff81d407f1 &lt;__alt_instructions_end+0x2c85&gt; ffffffff81d407ed: ff c9 dec %ecx ffffffff81d407ef: 75 f6 jne ffffffff81d407e7 &lt;__alt_instructions_end+0x2c7b&gt; ... ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this &lt;code&gt;objdump&lt;/code&gt; output, the following two lines are the default instructions from the &lt;code&gt;alternative_io&lt;/code&gt; macro:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ffffffff8131379e: ba 00 00 00 00 mov $0x0,%edx ffffffff813137a3: 66 66 66 90 data32 data32 xchg %ax,%ax &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And the &lt;code&gt;.altinstr_replacement&lt;/code&gt; section contains the instructions to use in their place if the hardware supports &lt;code&gt;RDRAND&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ffffffff81d407e7: 48 0f c7 f0 rdrand %rax ffffffff81d407eb: 72 04 jb ffffffff81d407f1 &lt;__alt_instructions_end+0x2c85&gt; ffffffff81d407ed: ff c9 dec %ecx ffffffff81d407ef: 75 f6 jne ffffffff81d407e7 &lt;__alt_instructions_end+0x2c7b&gt; &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Kernel in live RAM with RDRAND support&lt;/h3&gt; &lt;p&gt;Now we'll see the assembly language for the &lt;code&gt;get_random_int&lt;/code&gt; function in the running kernel. To be able to run the following command, install the &lt;a href="https://man7.org/linux/man-pages/man8/crash.8.html"&gt;&lt;code&gt;crash&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;gdb&lt;/code&gt;, and matching &lt;code&gt;kernel-debuginfo&lt;/code&gt; RPMs:&lt;/p&gt; &lt;pre&gt;&lt;code class="java bash numberLines"&gt;~]# crash -s /usr/lib/debug/lib/modules/2.6.32-279.el6.x86_64/vmlinux crash&gt; disassemble get_random_int Dump of assembler code for function get_random_int: 0xffffffff81313780 &lt;+0&gt;: push %rbp 0xffffffff81313781 &lt;+1&gt;: mov %rsp,%rbp 0xffffffff81313784 &lt;+4&gt;: sub $0x20,%rsp 0xffffffff81313788 &lt;+8&gt;: mov %rbx,-0x18(%rbp) 0xffffffff8131378c &lt;+12&gt;: mov %r12,-0x10(%rbp) 0xffffffff81313790 &lt;+16&gt;: mov %r13,-0x8(%rbp) 0xffffffff81313794 &lt;+20&gt;: nopl 0x0(%rax,%rax,1) 0xffffffff81313799 &lt;+25&gt;: mov $0xa,%edx 0xffffffff8131379e &lt;+30&gt;: rdrand %eax 0xffffffff813137a1 &lt;+33&gt;: jb 0xffffffff813137a7 &lt;get_random_int+39&gt; 0xffffffff813137a3 &lt;+35&gt;: dec %edx 0xffffffff813137a5 &lt;+37&gt;: jne 0xffffffff8131379e &lt;get_random_int+30&gt; ... End of assembler dump. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The kernel recognized at boot that it had &lt;code&gt;RDRAND&lt;/code&gt; support, so it inserted the four lines in the &lt;code&gt;.altinstr_replacement&lt;/code&gt; section.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;jc&lt;/code&gt;, &lt;code&gt;decl&lt;/code&gt;, and &lt;code&gt;jnz&lt;/code&gt; instructions were decoded by &lt;code&gt;crash&lt;/code&gt; into the functionally equivalent &lt;code&gt;jb&lt;/code&gt;, &lt;code&gt;dec&lt;/code&gt;, and &lt;code&gt;jne&lt;/code&gt; instructions, but that's a topic for another day.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article was a quick introduction to some of the tools I have used to quality test the Linux kernel in userspace.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/17/quality-testing-linux-kernel" title="Quality testing the Linux kernel"&gt;Quality testing the Linux kernel&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Jeff Bastian</dc:creator><dc:date>2022-02-17T07:00:00Z</dc:date></entry><entry><title type="html">Kubernetes Service Discovery and Selection with Stork</title><link rel="alternate" href="https://quarkus.io/blog/stork-kubernetes-discovery/" /><author><name>Aurea Munoz</name></author><id>https://quarkus.io/blog/stork-kubernetes-discovery/</id><updated>2022-02-17T00:00:00Z</updated><content type="html">As we already described in the previous post, SmallRye Stork is a service discovery and client-side load-balancing framework that brings out-of-the-box integration with Kubernetes, among others. This post will explain this integration, how to configure Stork in a client-side microservice, and how it differs from the classic Kubernetes service discovery...</content><dc:creator>Aurea Munoz</dc:creator></entry><entry><title>Deploy JBoss EAP with RHEL using the Azure Marketplace offering</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/16/deploy-jboss-eap-rhel-using-azure-marketplace-offering" /><author><name>Yeray Borges Santana</name></author><id>e8153815-69eb-4baf-89fc-b96faf422b51</id><updated>2022-02-16T07:00:00Z</updated><published>2022-02-16T07:00:00Z</published><summary type="html">&lt;p&gt;This article shows how to use the &lt;a href="https://azuremarketplace.microsoft.com/marketplace/apps/redhat.jboss-eap-rhel"&gt;Microsoft Azure Marketplace offering&lt;/a&gt; of &lt;a href="https://developers.redhat.com/products/eap/download"&gt;Red Hat JBoss Enterprise Application Platform (JBoss EAP)&lt;/a&gt; to deploy a JBoss EAP server running on &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux (RHEL)&lt;/a&gt; to Azure virtual machines (VMs). JBoss EAP is an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; application platform for building and deploying &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;enterprise Java&lt;/a&gt; applications. Azure is a very popular cloud service, and the JBoss EAP offering makes it easy to start using a powerful and familiar &lt;a data-linktype="external" href="https://jakarta.ee/"&gt;Jakarta EE&lt;/a&gt; runtime on Azure.&lt;/p&gt; &lt;h2&gt;JBoss Enterprise Application Platform on Azure Marketplace&lt;/h2&gt; &lt;p&gt;The Azure Marketplace offering of JBoss EAP offers three combinations of JBoss EAP and Red Hat Enterprise Linux:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;A standalone server running on an Azure Virtual Machine. This is the simplest case, with one JBoss EAP server running on Red Hat Enterprise Linux on a private virtual network with one subnet (Figure 1).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/standalone.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/standalone.png?itok=LSMSicbl" width="382" height="340" alt="The simplest Azure deployment is a standalone single JBoss EAP instance." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The simplest Azure deployment is a standalone single JBoss EAP instance. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;A cluster of JBoss EAP servers running on Azure VMs on a private virtual network with one subnet. You can choose the number of cluster members you want to deploy. This configuration uses an internal load balancer to distribute the incoming network traffic to your private network (Figure 2).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/multi.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/multi.png?itok=5e0nF98k" width="594" height="754" alt="Multiple JBoss EAP instances can run in a cluster with a load balancer to direct traffic." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. Multiple JBoss EAP instances can run in a cluster with a load balancer to direct traffic. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;An autoscale cluster server running on &lt;a href="https://azure.microsoft.com/en-us/services/virtual-machine-scale-sets/"&gt;Azure Virtual Machine Scale Set (VMSS)&lt;/a&gt;. This configuration is similar to the previous one, but can automatically increase or decrease the number of cluster members in response to demand (Figure 3).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/scale.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/scale.png?itok=0F9HdfoP" width="596" height="892" alt="Azure can also autoscale JBoss EAP instances." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. Azure can also autoscale JBoss EAP instances. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The example in this article uses the Red Hat offering on Azure to create a cluster of two JBoss EAP instances. We expose the instances to the internet behind an Azure firewall and get access to the JBoss management command-line interface (CLI) of each instance from your local machine.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;An Azure account with an active subscription: If you don't have an Azure subscription, you can &lt;a data-linktype="external" href="https://azure.microsoft.com/pricing/free-trial"&gt;create an account for free&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;JBoss EAP: You need a Red Hat account with a Red Hat Subscription Management entitlement for JBoss EAP. The entitlement lets you download a version of JBoss EAP tested and certified by Red Hat. If you don't have EAP entitlement, sign up for a free developer subscription through the &lt;a data-linktype="external" href="https://developers.redhat.com/register"&gt;Red Hat Developer Subscription for Individuals&lt;/a&gt;. Once registered, you can find the necessary credentials (Pool IDs) at the &lt;a data-linktype="external" href="https://access.redhat.com/management/"&gt;Red Hat Customer Portal&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The Azure CLI utility, which you can &lt;a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli"&gt;install from the Microsoft site&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Deploy a JBoss EAP cluster on an Azure VM&lt;/h2&gt; &lt;p&gt;The following steps describe how to deploy a JBoss EAP cluster on an Azure VM:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Get the Azure Marketplace offering of JBoss EAP on Red Hat Enterprise Linux. Open the Azure portal and select the &lt;strong&gt;Create a resource&lt;/strong&gt; button, and then search for &lt;strong&gt;Red Hat JBoss Enterprise Application Platform (JBoss EAP)&lt;/strong&gt;. You will arrive at the overview page in Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/offering.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/offering.png?itok=Hxe72C5h" width="1440" height="1084" alt="Azure has a JBoss EAP offering." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. Azure has a JBoss EAP offering. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Select your desired software plan, which defines the license model combination of JBoss EAP and Red Hat Enterprise Linux. At the moment of writing, the following license models are available:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Bring Your Own Subscription (BYOS): Use your existing Red Hat subscriptions to run Red Hat Products on Azure.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Pay As You Go (PYOS): Get billed periodically for usage by Microsoft.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;For this tutorial, we selected "JBoss EAP 7 (BYOS) on Red Hat Enterprise Linux 8 (PAYG) Clustered VM." Once you have selected your software plan, click the &lt;strong&gt;Create&lt;/strong&gt; button. You will be redirected to the &lt;strong&gt;Basics&lt;/strong&gt; configuration.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;You are going to connect to your cluster's VMs using an SSH public key, which you supply on the &lt;strong&gt;Basics&lt;/strong&gt; configuration tab. If you don't know how to create the SSH keys, you can follow the &lt;a href="http://go.microsoft.com/fwlink/?LinkId=2102401"&gt;Create and use an SSH public-private key pair for Linux VMs in Azure&lt;/a&gt; guide. As the terms indicate, you must keep your private key secret and use your public key on systems to which you want to connect.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;On the &lt;strong&gt;Basics&lt;/strong&gt; tab of the &lt;strong&gt;Create Red Hat JBoss Enterprise Application Platform (JBoss EAP)&lt;/strong&gt; page, enter or select the information listed in Table 1.&lt;/p&gt; &lt;table border="0" cellpadding="0" cellspacing="0" width="500"&gt;&lt;caption&gt;Table 1. Parameters for JBoss EAP instance.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th scope="col"&gt; &lt;p&gt;&lt;strong&gt;Setting&lt;/strong&gt;&lt;/p&gt; &lt;/th&gt; &lt;th scope="col"&gt; &lt;p&gt;&lt;strong&gt;Value&lt;/strong&gt;&lt;/p&gt; &lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;Subscription&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Select your Azure subscription.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;Resource group&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Select &lt;strong&gt;Create new&lt;/strong&gt; and enter &lt;code&gt;eap-cluster&lt;/code&gt; in the text box.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;Region&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Choose the Azure region that's right for you.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;Virtual Machine name&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Enter &lt;code&gt;eap-cluster-vm &lt;/code&gt;in the text box.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;Availability set name&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Enter &lt;code&gt;eap-cluster-as &lt;/code&gt;in the text box.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;Username&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;This is the Red Hat Enterprise Linux user account name. Enter &lt;code&gt;rheluser&lt;/code&gt;.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;Authentication type&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;This setting configures how your Red Hat Enterprise Linux user account can be authenticated in your system. Select &lt;strong&gt;SSH Public Key&lt;/strong&gt;.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;SSH public key&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;This is the SSH public key for the Virtual Machines. Paste here the content of the public key you generated earlier.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;Select the &lt;strong&gt;Next: Virtual Machine Settings&lt;/strong&gt; button to navigate to the next tab.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;On the &lt;strong&gt;Virtual Machine Settings&lt;/strong&gt; tab, configure the number of machines that will be part of your cluster, the size of your VMs, and the virtual network and subnet where your cluster will be deployed. We use all the defaults in this article except for the virtual network and subnet.&lt;/p&gt; &lt;p&gt;Click the &lt;strong&gt;Create new&lt;/strong&gt; button in the &lt;strong&gt;Virtual Network&lt;/strong&gt; field to open the &lt;strong&gt;Create virtual network&lt;/strong&gt; dialog. Edit the virtual network name and the subnet names as shown in Figure 5.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/create_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/create_1.png?itok=RlyLEpB-" width="1440" height="786" alt="Specify a name, subnet name, and address range." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5. Specify a name, subnet name, and address range. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;For the purposes of this example, the virtual network needs to have enough space to accommodate two subnets. One subnet will be private and the other will be public. At this step, you are defining only the private subnet. Later, when you add the Azure Firewall, you will need to create an additional subnet. So take both subnets into account at this stage. Use an address range for your virtual network and a private subnet large enough to allow you to create an additional subnet later.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Select the &lt;strong&gt;OK&lt;/strong&gt; button to accept the changes. The &lt;strong&gt;Create virtual network&lt;/strong&gt; dialog will close.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Click the &lt;strong&gt;Next: JBoss EAP Settings&lt;/strong&gt; button to navigate to the next tab.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;In the &lt;strong&gt;JBoss EAP Settings&lt;/strong&gt; tab, configure the JBoss EAP administrator username and password. Here you also supply information about the Red Hat subscription account that will be used to bring your license for your JBoss EAP product on Azure, in accordance with the BYOS model. Enter the information shown in Table 2.&lt;/p&gt; &lt;table border="0" cellpadding="0" cellspacing="0" width="500"&gt;&lt;caption&gt;Table 2. Parameters for JBoss EAP administrator.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th scope="col"&gt; &lt;p&gt;&lt;strong&gt;Setting&lt;/strong&gt;&lt;/p&gt; &lt;/th&gt; &lt;th scope="col"&gt; &lt;p&gt;&lt;strong&gt;Value&lt;/strong&gt;&lt;/p&gt; &lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;JBoss EAP Admin username&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Enter the name of the JBoss EAP administrator user: For example, &lt;code&gt;eapuser&lt;/code&gt;. You will log in to the JBoss EAP CLI later as this user.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;JBoss EAP password&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Enter the EAP admin password and confirm it. It must have a minimum of twelve characters.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;RHSM username&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Enter your Subscription Manager user account.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;RHSM password&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Enter your Subscription Manager password and confirm it.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;RHSM Pool ID with EAP entitlement&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Enter the Subscription Manager Pool ID where the JBoss EAP product is available.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;Select the &lt;strong&gt;Next: Review + create&lt;/strong&gt; button. All the supplied information will be immediately validated and a summary report will be shown.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;If you are fine with the summary, select the &lt;strong&gt;Create&lt;/strong&gt; button to deploy all the resources in Azure. The deployment might take a few minutes. Behind the scenes, the desired number of VMs will be created and the JBoss EAP server will be installed on those machines via RPM packages. Your Subscription Manager BYOS subscription will be used to download and install the JBoss EAP server on your Red Hat Enterprise Linux VMs. The default JBoss EAP server configuration will be modified accordingly, to make the server run properly on Azure.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;When your deployment finishes, you will see a summary of all the resources created automatically.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Verify the cluster data replication&lt;/h2&gt; &lt;p&gt;On each JBoss EAP server, the default multi-VM configuration of the Red Hat offering deploys a demo application that you can use to verify that your cluster is working properly. However, at this moment, we have two Red Hat Enterprise Linux virtual machines running on a private virtual network on Azure, and neither of those resources is accessible externally. In addition, the configured load balancer is an internal load balancer. It is not suitable for directly exposing your applications to external access by adding a public IP address. That restriction is intentional because it makes the default configuration secure without exposing any element to the outside world. It is up to you to decide what you want to expose and how. The best configuration choice depends considerably on your needs and what you are trying to achieve.&lt;/p&gt; &lt;p&gt;Assuming you want to access your cluster externally, in the following sections, we will explore one way to gain access to your JBoss instances by adding an Azure firewall.&lt;/p&gt; &lt;p&gt;If you don't want to expose your private network to the public, you have also multiple options: For example, you can &lt;a href="https://docs.microsoft.com/azure/virtual-machines/windows/quick-create-portal#create-virtual-machine"&gt;deploy a Windows VM&lt;/a&gt; on your private network and connect to it via &lt;a href="https://docs.microsoft.com/en-us/azure/bastion/bastion-overview"&gt;Azure Bastion&lt;/a&gt;. Once you have logged into your Windows machine, you will get access to your JBoss EAP cluster machines from there.&lt;/p&gt; &lt;h2&gt;Expose the cluster demo application to the internet&lt;/h2&gt; &lt;p&gt;Our goal is to get access from the public internet to the demo application deployed in your cluster. To do so, you will deploy an Azure firewall and place it in front of your internal load balancer so you can access your web applications externally in a secure way.&lt;/p&gt; &lt;p&gt;In the following sections, you will use the Azure CLI on a Linux machine to add and configure the required resources. In addition, you have to ensure that all your Azure CLI commands are created for the same location you used when you created the JBoss EAP cluster. You can handle the location by initializing an environment variable. That allows you to copy and paste the commands shown in this article without worrying about altering them to reflect your location.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; At the moment of writing, some commands related to the firewall configuration are in preview mode, so they might change slightly by the time you are reading this article.&lt;/p&gt; &lt;p&gt;This article assumes you have created an environment variable named &lt;code&gt;LOCATION&lt;/code&gt; to hold the location you used when creating your JBoss EAP resources. If you don't remember the location, grab it from your resource group by executing the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ LOCATION=$(az group show --name eap-cluster --query 'location' --output tsv)&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Set up the public IP address and firewall&lt;/h2&gt; &lt;p&gt;Azure Firewall is a managed, cloud-based network security service that protects your Azure Virtual Network resources. You have to create three sets of resources to make it work:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;A public IP address, to make your service accessible from the internet.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Security policies, which are collections of rules that allow inbound traffic to your Azure virtual network. These rules need to be added by creating three nested elements:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Rule collection groups: Sets of rule collections of any type. These are useful to group your rules in a logical way, based on a similar priority.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Rule collections: Sets of rules of the same type, which can apply to a DNAT, network, or application. We will add DNAT rules to translate the external requests arriving at the HTTP port.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Rules: Actual rules defining what to do when a request arrives at your public IP address. In this article, the security policy accepts TCP traffic that arrives at your public IP address at port 80, and translates the destination to the internal IP address of your load balancer, which is also listening at port 80. The load balancer redirects the incoming traffic from port 80 to port 8080 on each VM, choosing healthy VMs.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;The firewall resource itself, which is deployed on a different subnet. The firewall uses the defined security policies to check whether inbound or outbound traffic is allowed.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Create a public IP address&lt;/h3&gt; &lt;p&gt;Let's start by creating a public IP address:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Use &lt;a href="https://docs.microsoft.com/en-us/cli/azure/network/public-ip#az_network_public_ip_create"&gt;az network public-ip create&lt;/a&gt; to create the address: &lt;pre&gt; &lt;code class="language-bash"&gt;$ az network public-ip create \ --resource-group eap-cluster \ --location $LOCATION \ --name eap-cluster-public-ip \ --dns-name eapclusterdemo \ --sku Standard&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The DNS name assigned to the public IP address is &lt;code&gt;eapclusterdemo.$LOCATION.cloudapp.azure.com&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Create the firewall policy resource with &lt;a href="https://docs.microsoft.com/en-us/cli/azure/network/firewall/policy?view=azure-cli-latest#az_network_firewall_policy_create"&gt;az network firewall policy create&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ az network firewall policy create \ --name eap-cluster-fw-policy \ --resource-group eap-cluster \ --location $LOCATION&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Create a rule collection group to hold your rule collections with &lt;a href="https://docs.microsoft.com/en-us/cli/azure/network/firewall/policy/rule-collection-group?view=azure-cli-latest#az_network_firewall_policy_rule_collection_group_create"&gt;az network firewall policy rule-collection-group create&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ az network firewall policy rule-collection-group create \ --name eap-cluster-rule-collection-group \ --policy-name eap-cluster-fw-policy \ --priority 30000 \ --resource-group eap-cluster&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Create a DNAT rule collection to hold all the rules related to web traffic. In the same command, you can also create the rule itself. Our rule is pretty simple: It allows HTTP traffic from any IP address to our public IP address and translates the specified port from 80 to the internal load balancer IP address. To accomplish this transformation, you need to know the public IP address and the internal load balancer IP address. Grab them by using the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;LOAD_BALANCER_IP=$(az network lb show \ --name jbosseap-lb \ --resource-group eap-cluster \ --query 'frontendIpConfigurations[].privateIpAddress' --output tsv) AZURE_PUBLIC_IP=$(az network public-ip show \ --name eap-cluster-public-ip \ --resource-group eap-cluster \ --query 'ipAddress' --output tsv)&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Create the DNAT rule collection and rule with &lt;a href="https://docs.microsoft.com/en-us/cli/azure/network/firewall/policy/rule-collection-group/collection?view=azure-cli-latest#az_network_firewall_policy_rule_collection_group_collection_add_nat_collection"&gt;az network firewall policy rule-collection-group collection add-nat-collection&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ az network firewall policy rule-collection-group collection add-nat-collection \ --name http-web-collection \ --collection-priority 30000 \ --policy-name eap-cluster-fw-policy \ --resource-group eap-cluster \ --rule-collection-group-name eap-cluster-rule-collection-group \ --action DNAT \ --rule-name inbound_web_trafic \ --description "Allow inbound Web traffic to the internal Load Balancer" \ --source-addresses "*" \ --destination-addresses $AZURE_PUBLIC_IP \ --destination-ports 80 \ --translated-address $LOAD_BALANCER_IP \ --translated-port 80 \ --ip-protocols TCP&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;That's all you need to do regarding the firewall security policy. Now create the firewall resource itself with &lt;a href="https://docs.microsoft.com/en-us/cli/azure/network/firewall?view=azure-cli-latest#az_network_firewall_create"&gt;az network firewall create&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ az network firewall create \ --name eap-cluster-fw \ --resource-group eap-cluster \ --firewall-policy eap-cluster-fw-policy \ --location $LOCATION&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;To configure the firewall to protect your virtual network, you must create a subnet named &lt;strong&gt;AzureFirewallSubnet&lt;/strong&gt;. You can create it with &lt;a href="https://docs.microsoft.com/en-us/cli/azure/network/vnet/subnet?view=azure-cli-latest#az_network_vnet_subnet_create"&gt;az network vnet subnet create&lt;/a&gt;. At this step, pay attention to the address prefix you have to use. You probably will need to review what addresses you used when you were creating the JBoss EAP cluster. The subnet you need to create at this step is the public subnet:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ az network vnet subnet create \ --name AzureFirewallSubnet \ --resource-group eap-cluster \ --vnet-name eap-cluster-vnet \ --address-prefixes 10.0.1.0/24&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The firewall will be working on the 10.0.1.0/24 subnet, whereas your private subnet is on 10.0.0.0/24. The virtual network is 10.0.0.0/16.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Finally, configure the firewall to use the public IP address and your virtual network through &lt;a href="https://docs.microsoft.com/en-us/cli/azure/network/firewall/ip-config?view=azure-cli-latest#az_network_firewall_ip_config_create"&gt;az network firewall ip-config create&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ az network firewall ip-config create \ --firewall-name eap-cluster-fw \ --name eap-cluster-fw-ip \ --public-ip-address eap-cluster-public-ip \ --resource-group eap-cluster \ --vnet-name eap-cluster-vnet&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This operation takes a few minutes to complete. Once done, you will have access to the demo application deployed on the EAP servers.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Interact with the demo application&lt;/h2&gt; &lt;p&gt;At this point, your VMs are exposed on the internet, protected by a firewall. You should have access to them via the fully qualified domain name (FQDN) of your Azure Virtual Network public IP address. You can get the FQDN by issuing the following command:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ az network public-ip show \ --resource-group eap-cluster \ --name eap-cluster-public-ip \ --query 'dnsSettings.fqdn'&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Open your favorite browser and navigate to &lt;code&gt;http://&lt;DNS FQDN&gt;/eap-session-replication&lt;/code&gt;. The demo application's landing page will be displayed.&lt;/p&gt; &lt;p&gt;You can use this application to verify that the cluster is working as expected through the following steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Increment the counter several times. The demo application shows the VM's private IP address that served your last request (Figure 7).&lt;/li&gt; &lt;li&gt; &lt;p&gt;From the Azure portal, locate the VM with that IP address and stop it. You can find the VM's private IP address by inspecting the &lt;strong&gt;Networking&lt;/strong&gt; settings on your VM resource (Figure 6).&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="JBoss EAP VMo private IP" data-entity-type="file" data-entity-uuid="59ac40eb-baea-4ac7-965d-edd4b4af6561" src="https://developers.redhat.com/sites/default/files/inline-images/vm-0-networking.png" width="2390" height="922" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 6. The VM's resources page shows the public IP address in the Networking settings.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Go back to the demo application and select &lt;strong&gt;Refresh&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt; &lt;p&gt;You will get a response served by the other VM (Figure 8), which returns the same session ID and counter value.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Going through the steps in the previous list demonstrates that your session information is replicated in the cluster (Figures 7 and 8). If the process succeeds, restart the stopped VM so you can continue with the next section.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/demo0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/demo0.png?itok=7eswvvP8" width="600" height="776" alt="Azure displays session information for VM0." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7. Azure displays session information for VM0. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/demo1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/demo1.png?itok=pcc6AgIG" width="600" height="773" alt="Azure displays session information for VM1." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8. Azure displays session information for VM1. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Deploy your applications via the JBoss Management CLI&lt;/h2&gt; &lt;p&gt;In addition to the HTTP ports, you can add DNAT rules to access the JBoss EAP CLI on each server instance. Access to the CLI allows you to configure the servers further and deploy applications. Similar to what you did to expose the HTTP port, the following steps expose the SSH port of each VM through the firewall. You will use the private key of the SSH key pair you used when you created the cluster. To increase the level of security, you are allowing only connections coming from your local machine.&lt;/p&gt; &lt;p&gt;First of all, grab your public IP address (multiple sites on the internet such as &lt;a href="https://www.whatismyip.com/"&gt;whatismyip.com&lt;/a&gt; can give you this information), the public IP address of your cluster in Azure, and the private IP addresses of each VM. Save these addresses in variables, for instance:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;LOCAL_PUBLIC_IP="71.145.22.216" AZURE_PUBLIC_IP=$(az network public-ip show \ --name eap-cluster-public-ip \ --resource-group eap-cluster \ --query 'ipAddress' --output tsv) IP_VM0=$(az network nic show \ --name jbosseap-server-nic0 \ --resource-group eap-cluster \ --query 'ipConfigurations[].privateIpAddress' \ --output tsv) IP_VM1=$(az network nic show \ --name jbosseap-server-nic1 \ --resource-group eap-cluster \ --query 'ipConfigurations[].privateIpAddress' \ --output tsv)&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: By default, the network interfaces are created by appending the index of the VM that belongs to the cluster to &lt;code&gt;jbosseap-server-nic&lt;/code&gt; . Therefore, if you have created a cluster with two machines, the network interface names of those machines are &lt;code&gt;jbosseap-server-nic0&lt;/code&gt; and &lt;code&gt;jbosseap-server-nic1&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Add a new firewall rule collection to group the rules related to the SSH ports and two rules that translate the traffic coming from port 10022 to port 22 for the first Virtual Machine (VM0) and from port 20022 to port 22 for the other Virtual Machine (VM1). Notice that you are also specifying that the only valid source is your public IP address:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Create the rule for the VM0: &lt;pre&gt; &lt;code class="language-bash"&gt;$ az network firewall policy rule-collection-group collection add-nat-collection \ --name ssh-collection \ --collection-priority 30010 \ --policy-name eap-cluster-fw-policy \ --resource-group eap-cluster \ --rule-collection-group-name eap-cluster-rule-collection-group \ --action DNAT \ --rule-name inbound_ssh_trafic_vm0 \ --description "Allow inbound SSH traffic to VM 0" \ --source-addresses $LOCAL_PUBLIC_IP \ --destination-addresses $AZURE_PUBLIC_IP \ --destination-ports 10022 \ --translated-address $IP_VM0 \ --translated-port 22 \ --ip-protocols TCP&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Create the rule for the VM1: &lt;pre&gt; &lt;code class="language-bash"&gt;$ az network firewall policy rule-collection-group collection rule add \ --collection-name ssh-collection \ --name inbound_ssh_trafic_vm1 \ --policy-name eap-cluster-fw-policy \ --rule-collection-group-name eap-cluster-rule-collection-group \ --resource-group eap-cluster \ --rule-type NatRule \ --description "Allow inbound SSH traffic to VM 1" \ --source-addresses $LOCAL_PUBLIC_IP \ --destination-addresses $AZURE_PUBLIC_IP \ --destination-ports 20022 \ --translated-address $IP_VM1 \ --translated-port 22 \ --ip-protocols TCP&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Once you have created the rules, you can gain access to the VMs using SSH with the user name of your Red Hat Enterprise Linux VMs (&lt;code&gt;rheluser&lt;/code&gt; in this article) and your public key:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ssh -i eap-cluster-rsa rheluser@${AZURE_PUBLIC_IP} -p 10022 $ ssh -i eap-cluster-rsa rheluser@${AZURE_PUBLIC_IP} -p 20022&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And on each SSH session, you can get onto the JBoss CLI server by issuing the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[rheluser@eap-cluster-vm0 ~]$ $EAP_HOME/wildfly/bin/jboss-cli.sh -c -u=eapuser Authenticating against security realm: ManagementRealm Password: [standalone@localhost:9990 /]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This &lt;code&gt;eapuser&lt;/code&gt; user and its password are the credentials configured when you created the cluster.&lt;/p&gt; &lt;p&gt;Since you can connect to your VMs using SSH, you can also copy files securely using Secure Copy (&lt;code&gt;scp&lt;/code&gt;). To deploy an application in your cluster, you can copy the application to a known directory on the remote VMs, and then deploy the application by using the JBoss EAP CLI. For example, suppose you have a local application named &lt;code&gt;helloworld.war&lt;/code&gt;. You can deploy it by issuing the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ scp -i eap-cluster-rsa -P 10022 helloworld.war rheluser@${AZURE_PUBLIC_IP}:/tmp helloworld.war 100% 6495 109.1KB/s 00:00 $ ssh -i eap-cluster-rsa rheluser@${AZURE_PUBLIC_IP} -p 10022 [rheluser@eap-cluster-vm0 ~]$ $EAP_HOME/wildfly/bin/jboss-cli.sh -c -u=eapuser Authenticating against security realm: ManagementRealm Password: [standalone@localhost:9990 /] deploy /tmp/helloworld.war [standalone@localhost:9990 /] exit [rheluser@eap-cluster-vm0 ~]$ exit logout Connection to 20.90.187.110 closed.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Repeat the same commands for VM1 by using port 20022 in place of 10022. Your application will be deployed and available at &lt;code&gt;http://&lt;DNS FQDN&gt;/your-application-web-context&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;For debugging, you can find the server.log at &lt;code&gt;/opt/rh/eap7/root/usr/share/wildfly/standalone/log/server.log&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Clean up resources&lt;/h2&gt; &lt;p&gt;To delete all the resources created on Azure, open the &lt;strong&gt;eap-cluster&lt;/strong&gt; resource group and select &lt;strong&gt;Delete resource group&lt;/strong&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Red Hat accounts make it easy to use JBoss EAP on Azure with modern, production-level features such as load balancing. Try out different configurations and scale your applications.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/16/deploy-jboss-eap-rhel-using-azure-marketplace-offering" title="Deploy JBoss EAP with RHEL using the Azure Marketplace offering"&gt;Deploy JBoss EAP with RHEL using the Azure Marketplace offering&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Yeray Borges Santana</dc:creator><dc:date>2022-02-16T07:00:00Z</dc:date></entry><entry><title>Write Kubernetes in Java with the Java Operator SDK</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/15/write-kubernetes-java-java-operator-sdk" /><author><name>Christophe Laprun</name></author><id>1516637a-e905-4bc1-b6e6-ed887b069697</id><updated>2022-02-15T07:00:00Z</updated><published>2022-02-15T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://javaoperatorsdk.io"&gt;Java Operator SDK&lt;/a&gt;, or JOSDK, is an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; project that aims to simplify the task of creating &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; Operators using &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt;. The project was started by &lt;a href="https://container-solutions.com"&gt;Container Solutions&lt;/a&gt;, and Red Hat is now a major contributor.&lt;/p&gt; &lt;p&gt;In this article, you will get a brief overview of what Operators are and why it could be interesting to create them in Java. A future article will show you how to create a simple Operator using JOSDK.&lt;/p&gt; &lt;p&gt;As you can guess, this series of articles is principally targeted at Java developers interested in writing Operators in Java. You don't have to be an expert in Operators, Kubernetes, or &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt;. However, a basic understanding of all these topics will help. To learn more, I recommend reading Red Hat Developer's &lt;a href="https://developers.redhat.com/articles/2021/06/11/kubernetes-operators-101-part-1-overview-and-key-features"&gt;Kubernetes Operators 101 series&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Kubernetes Operators: A brief introduction&lt;/h2&gt; &lt;p&gt;Kubernetes has become the &lt;em&gt;de facto&lt;/em&gt; standard platform for deploying cloud applications. At its core, Kubernetes rests on a simple idea: The user communicates the state in which they want a cluster to be, and the platform will strive to realize that goal. A user doesn't need to tell Kubernetes the steps to get there; they just need to specify what that desired end state should look like. Typically, this involves providing the cluster with a materialized version of this desired state in the form of JSON or YAML files, sent to the cluster for consideration using the &lt;a href="https://kubernetes.io/docs/reference/kubectl/overview/"&gt;&lt;code&gt;kubectl&lt;/code&gt;&lt;/a&gt; tool. Assuming the desired state is valid, once it's on the cluster it will be handled by &lt;em&gt;controllers.&lt;/em&gt; Controllers are processes that run on the cluster and monitor the associated resources to reconcile their actual state with the state desired by the user.&lt;/p&gt; &lt;p&gt;Despite this conceptual simplicity, actually operating a Kubernetes cluster is not a trivial undertaking for non-expert users. Notably, deploying and configuring Kubernetes applications typically requires creating several resources, bound together by sometimes complex relations. In particular, developers who might not have experience on the operational side of things often struggle to move their applications from their local development environment to their final cloud destination. Reducing this complexity would therefore reap immense benefits for users, particularly by encapsulating the required operational knowledge in the form of &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; that could be interacted with at a higher level by users less familiar with the platform. This is what Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/"&gt;Operators&lt;/a&gt; were developed to achieve.&lt;/p&gt; &lt;h3&gt;Custom Resources&lt;/h3&gt; &lt;p&gt;Kubernetes comes with an extension mechanism in the form of &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"&gt;custom resources &lt;/a&gt; (CRs), which allow users to extend the Kubernetes platform in a way similar to how the core platform is implemented. There is not much formal difference between how native and custom resources are handled: both define domain-specific languages (DSLs) controlling one specific aspect of the platform realized by the YAML or JSON representations of the resources. While native resources control aspects that are part of the platform via their associated controllers, custom resources provide another layer on top of these native resources—allowing users to define higher-level abstractions, for example.&lt;/p&gt; &lt;p&gt;However, the platform doesn't know the first thing about these custom resources, so users must first register a controller with the platform to handle them. The combination of a custom resource-defined DSL and an associated controller enables users to define vocabularies that are closer to their business model. They can focus on business-specific aspects of their application rather than worrying about how a specific state will be realized on the cluster; the latter task falls under the responsibility of the associated controller. This pattern makes it possible to encapsulate the operational knowledge implemented by the associated controller behind the DSL provided by the custom resource. That's what Operators are: implementations of this useful pattern.&lt;/p&gt; &lt;p&gt;Operators are therefore quite attractive for those who want to reduce the knowledge required to deploy applications, but they also automate repetitive steps. They offer organizations the possibility of encapsulating business rules or processes behind a declarative "language" expressed by custom resources using a vocabulary tailored to the task at hand instead of dealing with Kubernetes-native resources that are foreign to less technical users. Once an Operator is installed and configured on a cluster, the logic and automation it provides are accessible to cluster users, who only have to deal with the associated DSL.&lt;/p&gt; &lt;h2&gt;Why write Operators in Java?&lt;/h2&gt; &lt;p&gt;Kubernetes and its ecosystem are written in the &lt;a href="https://developers.redhat.com/topics/go"&gt;Go programming language&lt;/a&gt;, and Operators traditionally have been as well. While it's not necessary to write everything in the same language, it's also a reality that the ecosystem is optimized for Go developers. To be fair, Go is well suited for this task: the language is relatively easy to learn and offers good runtime characteristics both in terms of memory and CPU usage. Moreover, several Go projects aim to make the Operator writing process easy:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://sdk.operatorframework.io/"&gt;&lt;code&gt;operator-sdk&lt;/code&gt;&lt;/a&gt; and its command line tool help developers get started faster&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/kubernetes/client-go/"&gt;&lt;code&gt;client-go&lt;/code&gt;&lt;/a&gt; facilitates programmatic interactions with the Kubernetes API server&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/kubernetes/apimachinery"&gt;&lt;code&gt;apimachinery&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://http://github.com/kubernetes/controller-runtime"&gt;&lt;code&gt;controller-runtime&lt;/code&gt;&lt;/a&gt; offer useful utilities and patterns&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;If Go is so good for writing Operators, why would anyone want to do it in Java? For one thing, Java is the language in which a significant number of enterprise applications are written. These applications are traditionally very complex by nature, and companies that rely on them would benefit from simplified ways to deploy and operate them at scale on Kubernetes clusters.&lt;/p&gt; &lt;p&gt;Moreover, the &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt; philosophy mandates that developers should also be responsible for deployment to production, maintenance, and other operational aspects of their application's lifecycle. From that perspective, being able to use the same language during all stages of the lifecycle is an attractive proposition.&lt;/p&gt; &lt;p&gt;Finally, Java-focused companies looking to write Kubernetes Operators want to capitalize on the existing wealth of Java experience among their developers. If developers can ramp up quickly in a programming language they already know rather than investing time and energy learning a new one, that offers a non-negligible advantage.&lt;/p&gt; &lt;h2&gt;Java in the cloud?&lt;/h2&gt; &lt;p&gt;That said, if writing Operators in Java offers so many benefits, why aren't more companies doing it? The first reason that comes to mind is that, compared to Go, Java has traditionally been pretty weak when it comes to deploying to the cloud. Indeed, Java is a platform that has been honed over decades for performance on long-running servers. In that context, memory usage or slow startup times are usually not an issue. This particular drawback has been progressively addressed over time, but the fact remains that a typical Java application will use more memory and start more slowly than a Go application. This matters quite a bit in a cloud environment, in which the pod where your application is running can be killed at any time (the &lt;a href="https://www.redhat.com/en/blog/container-tidbits-does-pets-vs-cattle-analogy-still-apply"&gt;cattle versus pet approach&lt;/a&gt;), and where you might need to scale up quickly (in &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;serverless&lt;/a&gt; environments in particular). Memory consumption also affects deployment density: the more memory your application consumes, the more difficult it is to deploy several instances of it on the same cluster where resources are limited.&lt;/p&gt; &lt;p&gt;Several projects have been initiated to improve Java's suitability for cloud environments, among which is &lt;a href="https://quarkus.io"&gt;Quarkus&lt;/a&gt;, on which this series of articles will focus. The Quarkus project describes itself as "a Kubernetes-native Java stack tailored for OpenJDK HotSpot and GraalVM, crafted from the best of breed Java libraries and standards." By moving much of the processing that is typically done by traditional Java stacks at runtime (e.g., annotation processing, properties file parsing, introspection) to build time, Quarkus improves Java application performance in terms of both consumed memory and startup time. By leveraging the &lt;a href="https://graalvm.org"&gt;GraalVM&lt;/a&gt; project, it also enables easier native compilation of Java applications, making them competitive with Go applications and almost removing runtime characteristics from the equation.&lt;/p&gt; &lt;h2&gt;What about framework support?&lt;/h2&gt; &lt;p&gt;However, as we've already noted, even if we're not taking runtime characteristics into account, Go is an attractive language in which to write Operators, thanks in no small part to the framework ecosystem it offers to support such a task. While there are Java clients that rival the &lt;code&gt;client-go&lt;/code&gt; project to help with interacting with the Kubernetes server, these clients only provide low-level abstractions, while the Go ecosystem provides higher-level frameworks and utilities targeted at Operator developers.&lt;/p&gt; &lt;p&gt;That's where JOSDK comes in, offering a framework comparable to what &lt;code&gt;controller-runtime&lt;/code&gt; offers to Go developers, but tailored for Java developers and using Java idioms. JOSDK aims to ease the task of developing Java Operators by providing a framework that deals with low-level events and implements best practices and patterns, thus allowing developers to focus on their Operator's business logic instead of worrying about the low-level operations required to interact with the Kubernetes API server.&lt;/p&gt; &lt;p&gt;Recognizing that Quarkus is particularly well suited for deploying Java applications, and more specifically Operators, in the cloud, Red Hat has taken JOSDK one step further by integrating it into &lt;a href="https://github.com/quarkiverse/quarkus-operator-sdk"&gt;&lt;code&gt;quarkus-operator-sdk&lt;/code&gt;, a Quarkus extension&lt;/a&gt; that simplifies the Java Operator development task even further by focusing on the development experience aspects. Red Hat has also contributed a plug-in for the &lt;a href="https://sdk.operatorframework.io/docs/cli/operator-sdk/"&gt;&lt;code&gt;operator-sdk&lt;/code&gt; command line tool&lt;/a&gt; to allow quick scaffolding of Java Operator projects using JOSDK and its Quarkus extension.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This concludes the first part of this series exploring writing Operators using JOSDK and Quarkus. You got a sense of the motivation for these projects and saw why it is interesting and useful to write Operators in Java.&lt;/p&gt; &lt;p&gt;In the next part of this series, you'll dive into JOSDK's concepts in greater detail and start implementing a Java Operator of your own using its Quarkus extension and the &lt;code&gt;operator-sdk&lt;/code&gt; command-line tool.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/15/write-kubernetes-java-java-operator-sdk" title="Write Kubernetes in Java with the Java Operator SDK"&gt;Write Kubernetes in Java with the Java Operator SDK&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Christophe Laprun</dc:creator><dc:date>2022-02-15T07:00:00Z</dc:date></entry><entry><title type="html">Getting started with Stork Service Discovery on Quarkus</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/quarkus/getting-started-with-stork-service-discovery-on-quarkus/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/quarkus/getting-started-with-stork-service-discovery-on-quarkus/</id><updated>2022-02-14T07:17:50Z</updated><content type="html">In modern microservices architectures, services have dynamically assigned locations. Therefore, it’s essential to integrate Service Discovery as part of the picture. In this article you will learn how to leverage Service Discovery using Smallrye Stork framework on top of a Quarkus reactive application. Service discovery in a nutshell Before we dig into this tutorial, we ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Serialize Debezium events with Apache Avro and OpenShift Service Registry</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/14/serialize-debezium-events-apache-avro-and-openshift-service-registry" /><author><name>Hugo Guerrero</name></author><id>491a421d-5483-4dde-9f1d-e32456a42240</id><updated>2022-02-14T07:00:00Z</updated><published>2022-02-14T07:00:00Z</published><summary type="html">&lt;p&gt;Change data capture (CDC) is a powerful data processing tool widely used in the industry, and provided by the open source&lt;a href="https://debezium.io/"&gt; Debezium&lt;/a&gt; project. CDC notifies your application whenever changes are made to a data set so that you can react promptly. This tutorial demonstrates how to use Debezium to monitor a MySQL database. As the data in the database changes, the resulting event streams are reflected in &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Debezium includes connectors for many types of data stores. In this tutorial, we will use the &lt;a href="https://debezium.io/documentation/reference/1.3/connectors/mysql.html"&gt;MySQL connector&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Red Hat OpenShift Service Registry&lt;/h2&gt; &lt;p&gt;This demo uses &lt;a href="https://www.redhat.com/es/technologies/cloud-computing/openshift/openshift-service-registry"&gt;Red Hat OpenShift Service Registry&lt;/a&gt;, a fully hosted and managed service that provides an &lt;a href="https://developers.redhat.com/topics/api-management/"&gt;API&lt;/a&gt; and schema registry for &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;. OpenShift Service Registry makes it easy for development teams to publish, discover, and reuse APIs and schemas.&lt;/p&gt; &lt;p&gt;The following services include OpenShift Service Registry at no additional charge:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://www.redhat.com/es/technologies/cloud-computing/openshift/openshift-api-management"&gt;Red Hat OpenShift API Management&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/es/technologies/cloud-computing/openshift/openshift-streams-for-apache-kafka"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Debezium schema serialization&lt;/h2&gt; &lt;p&gt;Although Debezium makes it easy to capture database changes and record them in &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Kafka&lt;/a&gt;, one of the critical decisions you have to make is &lt;em&gt;how&lt;/em&gt; you will serialize those change events in Kafka. Debezium allows you to select key and value &lt;em&gt;converters&lt;/em&gt; to choose from different types of options. OpenShift Service Registry enables you to store externalized schema versions to minimize the payload that is propagated.&lt;/p&gt; &lt;p&gt;By default, Debezium's converter includes the record's JSON message schema in each record, making the records very verbose. Alternatively, you can serialize the record keys and values using the compact binary format standardized in&lt;a href="https://avro.apache.org/"&gt; Apache Avro&lt;/a&gt;. To use Apache Avro serialization, you must deploy a schema registry that manages Avro message schemas and their versions.&lt;/p&gt; &lt;p&gt;OpenShift Service Registry provides an Avro converter that you can specify in Debezium connector configurations. This converter maps &lt;a href="https://docs.confluent.io/platform/current/connect/index.html"&gt;Kafka Connect&lt;/a&gt; schemas to Avro schemas. The converter then uses the Avro schemas to serialize the record keys and values into Avro's format.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;Install the following tools to run the tasks in this tutorial:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;The latest version of &lt;a href="https://www.docker.com/get-started"&gt;Docker&lt;/a&gt; or &lt;a href="https://podman.io/"&gt;Podman&lt;/a&gt;. (See the&lt;a href="https://docs.docker.com/engine/installation/"&gt; Docker Engine installation documentation&lt;/a&gt; or the &lt;a href="https://podman.io/getting-started/installation"&gt;Podman installation documentation&lt;/a&gt;.) &lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/edenhill/kcat"&gt;&lt;code&gt;kcat&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/kcctl/kcctl"&gt;&lt;code&gt;kcctl&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The &lt;a href="https://github.com/redhat-developer/app-services-cli/releases/latest"&gt;Red Hat Openshift Application Services CLI, &lt;code&gt;rhoas&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://stedolan.github.io/jq/"&gt;&lt;code&gt;jq&lt;/code&gt;&lt;/a&gt; (for JSON processing).&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You also need access to the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;A Red Hat Developer account. (As part of the developer program for OpenShift Streams for Apache Kafka, anyone with a Red Hat account can create a Kafka instance free of charge.)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;A running &lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka"&gt;OpenShift Streams cluster&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;A running &lt;a href="https://developers.redhat.com/articles/2021/10/11/get-started-openshift-service-registry"&gt;OpenShfit Service Registry instance&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Start the local services&lt;/h2&gt; &lt;p&gt;The MySQL database and the Kafka Connect cluster run locally on your machine for this demo. We will use &lt;a href="https://docs.docker.com/compose/"&gt;Docker Compose&lt;/a&gt; to start the required services, so there is no need to install anything beyond the prerequisites listed in the previous section.&lt;/p&gt; &lt;p&gt;To start the local services, follow these steps.&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Clone this repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone https://github.com/hguerrero/debezium-examples.git&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Change to the following directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd debezium-examples/debezium-openshift-registry-avro&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Open the &lt;code&gt;docker-compose.yaml&lt;/code&gt; file and edit the Kafka-related properties to specify your cluster information. You need to know the name of your Kafka bootstrap server and the service account you will use to connect. The container image will then take the password from a local file called &lt;code&gt;cpass&lt;/code&gt;. Thus, your properties should look like:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; KAFKA_CONNECT_BOOTSTRAP_SERVERS: &lt;your-boostrap-server&gt;:&lt;port&gt; KAFKA_CONNECT_TLS: 'true' KAFKA_CONNECT_SASL_MECHANISM: plain KAFKA_CONNECT_SASL_USERNAME: &lt;kafka-sa-client-id&gt; KAFKA_CONNECT_SASL_PASSWORD_FILE: cpass&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Open the provided &lt;code&gt;cpass&lt;/code&gt; file and replace the placeholder with your service account secret.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Start the environment using one of these two commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman-compose up -d&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;docker-compose up -d&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The preceding command starts the following components:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A single-node Kafka Connect cluster&lt;/li&gt; &lt;li&gt;The MySQL database (ready for CDC)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Configure Apicurio converters&lt;/h2&gt; &lt;p&gt;The open source&lt;a href="https://www.apicur.io/registry/"&gt; Apicurio Registry project&lt;/a&gt; is the upstream community that furnishes the technology used by OpenShift Service Registry. Apicurio Registry provides Kafka Connect converters for Apache Avro and JSON Schema. When configuring Avro at the Debezium Connector, you have to specify the converter and schema registry as a part of the connector's configuration. The connector configuration customizes the connector, explicitly setting its serializers and deserializers to use Avro and specifying the location of the Apicurio registry.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/topics/containers/"&gt;container&lt;/a&gt; image used in this environment includes all the required libraries to gain access to the Debezium connectors and Apicurio Registry converters.&lt;/p&gt; &lt;p&gt;The following snippet contains the lines required in the connector configuration to set the &lt;code&gt;key&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; converters and their respective registry configuration. Replace the placeholders in the snippet with the information from your OpenShift services:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;"key.converter": "io.apicurio.registry.utils.converter.AvroConverter", "key.converter.apicurio.registry.converter.serializer": "io.apicurio.registry.serde.avro.AvroKafkaSerializer", "key.converter.apicurio.registry.url": "&lt;your-service-registry-core-api-url&gt;", "key.converter.apicurio.auth.service.url": "https://identity.api.openshift.com/auth", "key.converter.apicurio.auth.realm": "rhoas", "key.converter.apicurio.auth.client.id": "&lt;registry-sa-client-id&gt;", "key.converter.apicurio.auth.client.secret": "&lt;registry-sa-client-id&gt;", "key.converter.apicurio.registry.as-confluent": "true", "key.converter.apicurio.registry.auto-register": "true", "value.converter": "io.apicurio.registry.utils.converter.AvroConverter", "value.converter.apicurio.registry.converter.serializer": "io.apicurio.registry.serde.avro.AvroKafkaSerializer", "value.converter.apicurio.registry.url": "&lt;your-service-registry-core-api-url&gt;", "value.converter.apicurio.auth.service.url": "https://identity.api.openshift.com/auth", "value.converter.apicurio.auth.realm": "rhoas", "value.converter.apicurio.auth.client.id": "&lt;registry-sa-client-id&gt;", "value.converter.apicurio.auth.client.secret": "&lt;registry-sa-client-id&gt;", "value.converter.apicurio.registry.as-confluent": "true", "value.converter.apicurio.registry.auto-register": "true"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The compatibility mode allows you to use other providers' tooling to deserialize and reuse the schemas in the Apicurio service registry.&lt;/p&gt; &lt;p&gt;This configuration also includes the information required for the serializer to authenticate with the service registry using a service account.&lt;/p&gt; &lt;h2&gt;Create the topics in OpenShift Streams for Apache Kafka&lt;/h2&gt; &lt;p&gt;You need to manually create the required topics Debezium will use in your Kafka cluster. The user interface (UI) in OpenShift Streams for Apache Kafka makes configuration easier and eliminates some sources of errors. The recommended parameters for each topic are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Partitions: &lt;code&gt;1&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Retention time: &lt;code&gt;604800000 ms (7 days)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Retention size: &lt;code&gt;Unlimited&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;strong&gt;T&lt;/strong&gt;he topics whose names start with &lt;code&gt;debezium-cluster-&lt;/code&gt; must also be configured with the &lt;code&gt;compact&lt;/code&gt; policy (Figure 1). If you don't set this property correctly, the connector won't be able to start and errors will appear in the Kafka Connect log.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/cleanup-policy-debezium.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/cleanup-policy-debezium.png?itok=-1Hzd0_k" width="1440" height="763" alt="The cleanup policy must be "compact."" loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The cleanup policy must be "compact." &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The topics to create are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;avro&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;avro.inventory.addresses&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;avro.inventory.customers&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;avro.inventory.geom&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;avro.inventory.orders)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;avro.inventory.products&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;avro.inventory.products_on_hand&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;debezium-cluster-configs&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;debezium-cluster-offsets&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;debezium-cluster-status&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;schema-changes.inventory&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The resulting table of topics should look like Figure 2.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/topics-openshift-streams-debezium.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/topics-openshift-streams-debezium.png?itok=WhHlQzfy" width="1440" height="667" alt="When configured correctly, all the topics show up in the user interface." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. When configured correctly, all the topics show up in the user interface. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Configure the database history&lt;/h2&gt; &lt;p&gt;In a separate &lt;a href="https://debezium.io/documentation/reference/stable/connectors/mysql.html#mysql-schema-history-topic"&gt;database history Kafka topic&lt;/a&gt;, the Debezium connector for MySQL records all data definition language (DDL) statements along with the position in the binlog where each DDL statement appears. In order to store that information, the connector needs access to the target Kafka cluster, so you need to add the connection details to the connector configuration.&lt;/p&gt; &lt;p&gt;Create the following lines, inserting the correct values for your environment in your connector configuration to access OpenShift Streams for Apache Kafka. Add the lines as you did with the details of the converter. You need to configure the producer and consumer authentication independently:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;"database.history.kafka.topic": "schema-changes.inventory", "database.history.kafka.bootstrap.servers": "&lt;your-boostrap-server&gt;", "database.history.producer.security.protocol": "SASL_SSL", "database.history.producer.sasl.mechanism": "PLAIN", "database.history.producer.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=&lt;kafka-sa-client-id&gt; password=&lt;kafka-sa-client-secret&gt;;", "database.history.consumer.security.protocol": "SASL_SSL", "database.history.consumer.sasl.mechanism": "PLAIN", "database.history.consumer.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=&lt;kafka-sa-client-id&gt; password=&lt;kafka-sa-client-secret&gt;;",&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can check the final file configuration by viewing the &lt;code&gt;dbz-mysql-openshift-registry-avro.json&lt;/code&gt; file under the main folder.&lt;/p&gt; &lt;h2&gt;Create the connector&lt;/h2&gt; &lt;p&gt;Now that the configuration for the connector is ready, add the configuration to the Kafka Connect cluster so that it starts the task that captures changes to the database. Use the &lt;code&gt;kcctl&lt;/code&gt; command-line client for Kafka Connect, which allows you to register and examine connectors, delete them, and restart them, among other features.&lt;/p&gt; &lt;p&gt;Configure the &lt;code&gt;kcctl&lt;/code&gt; context:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kcctl config set-context --cluster http://localhost:8083 local&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Register the connector:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kcctl apply -f dbz-mysql-openshift-registry-avro.json&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Check the registry&lt;/h2&gt; &lt;p&gt;Go to the OpenShift Service Registry console. There you should find all the schema artifacts, as shown in Figure 3.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/registry-debezium-artifacts.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/registry-debezium-artifacts.png?itok=5X4IlJI9" width="1145" height="705" alt="The OpenShift Service Registry console shows all the artifacts created for Debezium." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. The OpenShift Service Registry console shows all the artifacts created for Debezium. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Check the data&lt;/h2&gt; &lt;p&gt;Now use the &lt;code&gt;kcat&lt;/code&gt; command-line utility to query the information stored in the OpenShift Streams Kafka cluster:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Set the environment variables in your terminal session to specify your cluster information:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export BOOTSTRAP_SERVER=&lt;replace-with-bootstrap-server&gt; $ export CLIENT_ID=&lt;replace-with-kafka-sa-client-id&gt; $ export CLIENT_SECRET=&lt;replace-with-kafka-sa-client-secret&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Check connectivity by querying the cluster metadata:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kcat -b $BOOTSTRAP_SERVER \ -X sasl.mechanisms=PLAIN \ -X security.protocol=SASL_SSL \ -X sasl.username="$CLIENT_ID" \ -X sasl.password="$CLIENT_SECRET" -L&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should get an output similar to the following:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Metadata for all topics (from broker -1: sasl_ssl://kafkaesque-c-isn-bhfjlsl-g-dana.bf2.kafka.rhcloud.com:443/bootstrap): 3 brokers: broker 0 at broker-0-kafkaesque-c-isn-bhfjlsl-g-dana.bf2.kafka.rhcloud.com:443 (controller) broker 2 at broker-2-kafkaesque-c-isn-bhfjlsl-g-dana.bf2.kafka.rhcloud.com:443 broker 1 at broker-1-kafkaesque-c-isn-bhfjlsl-g-dana.bf2.kafka.rhcloud.com:443 11 topics: topic "avro.inventory.orders" with 1 partitions: partition 0, leader 0, replicas: 0,1,2, isrs: 0,1,2 topic "avro.inventory.addresses" with 1 partitions: partition 0, leader 2, replicas: 2,0,1, isrs: 2,0,1 topic "debezium-cluster-configs" with 1 partitions: partition 0, leader 2, replicas: 2,0,1, isrs: 2,0,1 topic "debezium-cluster-offsets" with 1 partitions: partition 0, leader 0, replicas: 0,1,2, isrs: 0,1,2 topic "avro.inventory.products" with 1 partitions: partition 0, leader 0, replicas: 0,2,1, isrs: 0,2,1 topic "debezium-cluster-status" with 1 partitions: partition 0, leader 0, replicas: 0,2,1, isrs: 0,2,1 topic "avro" with 1 partitions: partition 0, leader 1, replicas: 1,2,0, isrs: 1,2,0 topic "avro.inventory.geom" with 1 partitions: partition 0, leader 0, replicas: 0,2,1, isrs: 0,2,1 topic "schema-changes.inventory" with 1 partitions: partition 0, leader 2, replicas: 2,0,1, isrs: 2,0,1 topic "avro.inventory.products_on_hand" with 1 partitions: partition 0, leader 0, replicas: 0,2,1, isrs: 0,2,1 topic "avro.inventory.customers" with 1 partitions: partition 0, leader 1, replicas: 1,2,0, isrs: 1,2,0&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Now check the records in the &lt;code&gt;customers&lt;/code&gt; topic:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kcat -b $BOOTSTRAP_SERVER \ -X sasl.mechanisms=PLAIN \ -X security.protocol=SASL_SSL \ -X sasl.username="$CLIENT_ID" \ -X sasl.password="$CLIENT_SECRET" \ -t avro.inventory.customers -C -e&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should see the following four scrambled records in the terminal:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;� Sally Thomas*sally.thomas@acme.com01.5.4.Final-redhat-00001 mysqavro����trueinventorycustomers mysql-bin.000003�r����_ � George Bailey$gbailey@foobar.com01.5.4.Final-redhat-00001 mysqavro����trueinventorycustomers mysql-bin.000003�r����_ � Edward Walkered@walker.com01.5.4.Final-redhat-00001 mysqavro����trueinventorycustomers mysql-bin.000003�r����_ AnneKretchmar$annek@noanswer.org01.5.4.Final-redhat-00001 mysqavro����lastinventorycustomers mysql-bin.000003�r����_ % Reached end of topic avro.inventory.customers [0] at offset 4&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The garbled formatting shows up because we are using Avro for serialization. The &lt;code&gt;kcat&lt;/code&gt; utility expects text strings and hence cannot convert the format correctly. The following step fixes the problem.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Ask &lt;code&gt;kcat&lt;/code&gt; to connect with the OpenShift Service Registry so it can query the schema used and correctly deserialize the Avro records. OpenShift Service Registry supports various types of authentication; the following command uses basic authentication, specifying credentials in the format &lt;code&gt;https://&lt;username&gt;:&lt;password&gt;@&lt;URL&gt;&lt;/code&gt;.:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kcat -b $BOOTSTRAP_SERVER \ -X sasl.mechanisms=PLAIN \ -X security.protocol=SASL_SSL \ -X sasl.username="$CLIENT_ID" \ -X sasl.password="$CLIENT_SECRET" \ -t avro.inventory.customers -C -e \ -s avro -r https://&lt;registry-sa-client-id&gt;:&lt;registry-sa-client-secret&gt;@&lt;registry-compatibility-api-url&gt; | jq&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now the records are displayed in a nicely formatted JSON structure:&lt;/p&gt; &lt;pre&gt; &lt;code class="javascript"&gt;... { "before": null, "after": { "Value": { "id": 1004, "first_name": "Anne", "last_name": "Kretchmar", "email": "annek@noanswer.org" } }, "source": { "version": "1.5.4.Final-redhat-00001", "connector": "mysql", "name": "avro", "ts_ms": 1642452727355, "snapshot": { "string": "last" }, "db": "inventory", "sequence": null, "table": { "string": "customers" }, "server_id": 0, "gtid": null, "file": "mysql-bin.000003", "pos": 154, "row": 0, "thread": null, "query": null }, "op": "r", "ts_ms": { "long": 1642452727355 }, "transaction": null }&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Congratulations! You can send Avro serialized records from MySQL to OpenShift Streams for Apache Kafka using OpenShift Service Registry. Visit the following links to learn more:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka"&gt;Get started with Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/10/11/get-started-openshift-service-registry"&gt;Getting started with OpenShfit Service Registry&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/videos/youtube/QYbXDp4Vu-8"&gt;Apache Kafka and Debezium | DevNation Tech Talk&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/topics/event-driven/connectors"&gt;Debezium Apache Kafka connectors&lt;/a&gt; from &lt;a href="https://developers.redhat.com/integration"&gt;Red Hat Integration&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/14/serialize-debezium-events-apache-avro-and-openshift-service-registry" title="Serialize Debezium events with Apache Avro and OpenShift Service Registry"&gt;Serialize Debezium events with Apache Avro and OpenShift Service Registry&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Hugo Guerrero</dc:creator><dc:date>2022-02-14T07:00:00Z</dc:date></entry><entry><title type="html">DevOpsDays Raleigh 2022 - Talking Architecture and Career</title><link rel="alternate" href="http://www.schabell.org/2022/02/devopsdays-raleigh-2022-architecture-and-career.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/02/devopsdays-raleigh-2022-architecture-and-career.html</id><updated>2022-02-14T06:00:00Z</updated><content type="html"> It's been some years since , mostly due to the worldly situation and travel restrictions, but thought this year was a great time to jump back in. The 2022 edition will be held on April 13-14 in Raleigh, NC. You can and join the fun for a really interesting array of talks and workshops.  I'm going to give it a good shot this year and have submitted two sessions and a workshop.  The first session is a simple and yet motivational talk based on a theme that I've used before in keynotes to open several other conferences. It's relevant to all parts of our industry so I've pushed it here with the focus on DevOps. OPEN IS KEY TO YOUR DEVOPS CAREER It's not coincidence. It's not luck. It's not going to happen by itself, so what's the secret sauce? Understanding what makes a DevOps career in open source grow, what choices are crucial, and what actions accelerate or damage your future are sometimes hard to grasp. Learning to position, expand and grow your personal DevOps brand in the open source world is what this session provides. Be ready for your next step in open source. Join me for a story sharing a clear and easy to use plan for jump starting your DevOps open source career immediately. The next session is more from my series called Talking Architecture Shop. This again will focus on architecture research for solutions in the DevOps domain that scale.  TALKING ARCHITECTURE SHOP - EXPLORING OPEN SOURCE DEVOPS AT SCALE  You've heard of large scale open source architectures, but have you ever wanted to take a serious look at these real life enterprise DevOps implementations that scale? This session takes attendees on a tour of multiple use cases covering DevOps challenges with hybrid cloud management with GitOps, DevOps in healthcare, and much more. Not only are these architectures interesting, but they are successful real life implementations featuring open source technologies and power many of your own online experiences. The attendee departs this session with a working knowledge of how to map general open source technologies to their solutions. Material covered is available freely online and attendees can use these solutions as starting points for aligning to their own solution architectures. Join us for an hour of power as we talk architecture shop!  Finally, nothing beats a hands-on workshop, so I shared the DevOps Heroes workshop that I presented back in 2019, but updated for this go around. CREATING REAL DEVOPS HEROES - ADDING PROCESS AUTOMATION TO TOOLBOX DevOps is more than the process of automating your CI/CD pipelines to generate code and deployment artefacts for production. It's also about organizational change and integration of many subtle processes that help you to deliver applications seamlessly from development to production through your operations. Let's unlock the power of process integration with a hands-on workshop using your own devices (laptops). We'll take you through the integration of an organizational process as part of your DevOps strategy. Step-by-step you'll build a domain model, creating an automated process, integrating user approval tasks and more using modern open source process automation tooling.  Bring your laptop as this is a hands on experience that takes you from nothing to a fully working DevOps supporting automation integration project. No experience in automation integration is required. Let's add a new tool to your development toolbox and get you jump started on automation integration that's supporting your organizations DevOps activities. Fingers crossed that the selection committee likes what they see and we get invited to meet with you face to face. It's about time, don't you think?</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">Keycloak 17.0.0 released</title><link rel="alternate" href="https://www.keycloak.org/2022/02/keycloak-1700-released" /><author><name /></author><id>https://www.keycloak.org/2022/02/keycloak-1700-released</id><updated>2022-02-11T00:00:00Z</updated><content type="html">To download the release go to . RELEASE NOTES HIGHLIGHTS QUARKUS DISTRIBUTION IS NOW FULLY SUPPORTED The default Keycloak distribution is now based on Quarkus. The new distribution is faster, leaner, and a lot easier to configure! We appreciate migrating from the WildFly distribution is not going to be straightforward for everyone, since how you start and configure Keycloak has radically changed. With that in mind we will continue to support the WildFly distribution until June 2022. For information on how to migrate to the new distribution check out the . QUARKUS DISTRIBUTION UPDATES A lot of effort went into polishing and improving the Quarkus distribution to make it as good as an experience as possible. A few highlights include: * A new approach to documentation in form of server guides to help you install and configure Keycloak * Upgraded Quarkus to 2.7.0.Final * Configuration file is on longer Java specific, and aligns configuration keys with CLI arguments * Clearer separation between build options and runtime configuration. * h2-mem and h2-file databases renamed to dev-mem and dev-file. * Simplified enabling and disabling features * Custom, and unsupported, Quarkus configuration is done through conf/quarkus.properties. * Ability to add custom Java Options via JAVA_OPTS_APPEND (thanks to ) * Initial logging capabilities * Initial support for Cross-DC * User-defined profiles are no longer supported but using different configuration files to achieve the same goal * Quickstarts updated to use the new distribution == Other improvements OFFLINE SESSIONS LAZY LOADED The offline sessions are now lazily fetched from the database by default instead of preloading during the server startup. To change the default behavior, see . IMPROVED USER SEARCH Keycloak now supports a glob-like syntax for the user search when listing users in the Admin Console, which allows for three different types of searches: prefix (foo* which became the default search), infix (*foo*), and exact "foo") MIGRATION FROM 16.1 Before you upgrade remember to backup your database. If you are not on the previous release refer to for a complete list of migration changes. DEFAULT DISTRIBUTION IS NOW POWERED BY QUARKUS The default distribution of Keycloak is now powered by Quarkus, which brings a number of breaking changes to you configure Keycloak and deploy custom providers. For more information check out the . The WildFly distribution of Keycloak is now deprecated, with support ending June 2022. We recommend migrating to the Quarkus distribution as soon as possible. However, if you need to remain on the legacy WildFly distribution for some time, there are some changes to consider: * Container images for the legacy distribution tags have changed. To use the legacy distribution use the tags legacy or 17.0.0-legacy. * Download on the website for the legacy distribution has changed to keycloak-legacy-17.0.0.[zip|tar.gz]. If you encounter problems migrating to the Quarkus distribution, missing ability to configure something, or have general ideas and feedback, please open a discussion in . MIGRATING FROM THE PREVIEW QUARKUS DISTRIBUTION A number of things have changed since the preview Quarkus distribution was released in Keycloak 15.1.0. The ideal way to learn about what’s changed is to check out the new . In summary, the changes include: * Container now published to quay.io/keycloak/keycloak:latest and quay.io/keycloak/keycloak:17.0.0 * Download on website renamed to keycloak-17.0.0.[zip|tar.gz]. * conf/keycloak.properties changed to conf/keycloak.conf, which unifies configuration keys between the config file and CLI arguments. * Clearer separation between build options and runtime configuration. * Custom Quarkus configuration is done through conf/quarkus.properties. * h2-mem and h2-file databases renamed to dev-mem and dev-file. * Features are now enabled/disabled with --features and --features-disabled replacing the previous approach that had an separate config key for each feature. * Runtime configuration can no longer be passed to kc.[sh|bat] build and is no longer persisted in the build * Logging level and format is now configured with --log-level and --log-format, while in the past these had to be configured using unsupported Quarkus properties. CLIENT POLICIES MIGRATION : CLIENT-SCOPES If you used a policy including client-scopes condition and edited JSON document directly, you will need to change the "scope" field name in a JSON document to "scopes". LIQUIBASE UPGRADED TO VERSION 4.6.2 Liquibase was updated from version 3.5.5 to 4.6.2, which includes, among other things, several bug fixes, and a new way of registering custom extensions using ServiceLoader. Migration from previous Keycloak versions to Keycloak 17.0.0 has been extensively tested with all currently supported databases, but we would like to stress the importance of closely following the , specifically of backing up existing database before upgrade. While we did our best to test the consequences of the Liquibase upgrade, some installations could be using specific setup unknown to us. ALL RESOLVED ISSUES NEW FEATURES * Convert MapUserEntity to interface keycloak storage * Create Operator.X module in repo keycloak operator * Keycloak.X deployment keycloak operator * Realm CRD keycloak operator * Testsuite baseline keycloak operator * Let users configure Dynamic Client Scopes keycloak * Create an internal representation of RAR that also handles Static and Dynamic Client Scopes keycloak * Handle Dynamic Scopes correctly in the consent screen keycloak * Publish ECMAScript Modules for keycloak-js keycloak adapter/javascript * Package server guides to be used in the website keycloak dist/quarkus * JPA map storage: Client scope no-downtime store keycloak storage * Convert authorization services entities into interface keycloak storage * Generate the CRD from RealmRepresentation keycloak operator * Improve user search query keycloak storage * Configurable session limits keycloak authentication ENHANCEMENTS * Update Kubernetes and OpenShift examples used by getting started guides to use Quarkus dist keycloak-quickstarts * Update getting-started in QuickStarts to use Quarkus dist keycloak-quickstarts * Update default container to Quarkus keycloak-containers * Update legacy image coordinates in the image manager keycloak-operator * Update documentation for Quarkus distribution keycloak-documentation * Release notes for Keycloak 17 keycloak-documentation * Migration from Keycloak.X preview keycloak-documentation * Documentation for Quarkus distribution keycloak dist/quarkus * Disable pre-loading offline sessions by default keycloak dist/quarkus * Remove Hashicorp Support keycloak dist/quarkus * Make JsonbType generic keycloak storage * Tree storage: introduce notion of per-field primary and cached status in an entity keycloak storage * Provide documentation for proxy mode in Quarkus based Keycloak keycloak dist/quarkus * Review README files. keycloak dist/quarkus * Add indexing to HotRodGroupEntity keycloak storage * Make JpaClientStorage* classes generic keycloak storage * Refactor generated constructors in new store entities keycloak storage * Avoid building configuration all the time when running tests keycloak * Remove override on xmlsec in quarkus/pom.xml keycloak dist/quarkus * Database configuration tests keycloak * Upgrade Infinispan to 12.1.7.Final keycloak storage * Cross-site validation for lazy loading of offline sessions keycloak storage * Switch default offline sessions to lazy loaded keycloak docs * HotRod map storage uses regex pattern that could be precompiled keycloak storage * Add configuration guide keycloak dist/quarkus * Validation for CIBA binding_message parameter keycloak * Upgrade Liquibase to 4.6.2 keycloak storage * Add more details about 2FA to authenticate page keycloak * Verify the WebAuthn functionality and settings for authentication keycloak testsuite * Enable only TLSv1.3 as default for the https protocol and set expected values keycloak dist/quarkus * Backward compatibility for lower-case bearer type in token responses keycloak * Test scenarios for verifying of JS injection for WebAuthn Policy keycloak testsuite * Improve the kustomize setup for the operator keycloak operator * Readiness and Liveness probe for the operator deployment of Keycloak.X keycloak operator * Multiple warnings caused by typed varargs in TokenVerifier keycloak * optimize title/summary and headlines for proxy guide keycloak dist/quarkus * Add support to linking between guides keycloak docs * Remove output of summary in guides keycloak docs * Build command should only accept built-time options keycloak dist/quarkus * Exclude some folders from our SAST analysis keycloak * Convert MapClientScopeEntity to interface keycloak storage * Add a quarkus.properties for unsupported configuration options keycloak dist/quarkus * Remove any reference to configuration profile keycloak dist/quarkus * Remove system property from help message keycloak dist/quarkus * Hide Hasicorp Vault from CLI keycloak dist/quarkus * Improve enabling/disabling features in Quarkus distribution keycloak dist/quarkus * Device Authorization Grant with PKCE keycloak * Adpaters for Map Storage swallow multi-valued attribute while Keycloak Core doesn't support them keycloak storage * Restrict Dynamic Scopes to optional Client Scopes keycloak * Add section recommended exposed paths to reverse proxy documentation keycloak docs * Combine package files for JS adapter keycloak adapter/javascript * Add test scenarios for Passwordless Webauthn AIA keycloak testsuite * Extend and fix tests for Resident Keys for WebAuthn keycloak testsuite * Store information about transport media of WebAuthn authenticator keycloak authentication/webauthn * Sort options in guides by key keycloak docs * Update default ZIP distribution to Quarkus keycloak dist/wildfly * Complete support for Passwordless tests keycloak testsuite * Use keycloak.v2 admin theme by default if admin2 is enabled keycloak admin/ui * Quarkus update to 2.7.0 Final keycloak dist/quarkus * Initial logging support keycloak dist/quarkus * Add support for pinning guides to the top keycloak docs * Implement the Dynamic Scopes parsing for the resource-owner-password-credentials grant. keycloak oidc * Verify if enabling authentication and encryption for JGroups work on Quarkus dist keycloak dist/quarkus * Add note about escaping of vaules for config keycloak dist/quarkus * Logging guide for Quarkus dist keycloak dist/quarkus * Update com.github.ua-parser:uap-java to 1.5.2 keycloak dependencies * Remove external Collection utility class for WebAuthn keycloak authentication/webauthn * Cover enabling mtls in TLS guide keycloak dist/quarkus * Updated use of generics in JPA Map Storage keycloak storage * Create common parent for Jpa*AttributeEntity keycloak storage * Reduce Keycloak.x image size keycloak dist/quarkus BUGS * Incorrect dependency in package.json keycloak-nodejs-connect * KeycloakAuthenticatorValve (Tomcat) does not implement createRequestAuthenticator() keycloak adapter/jee * Spurious logs are spilling in Quarkus Distribution.X integration tests keycloak dist/quarkus * The title of the login screen is not translated into Japanese keycloak * Quarkus relational database setup documentation error keycloak * "look-ahead window" of TOTP should be "look around window" keycloak * Expected Scopes of ClientScopesCondition created on Admin UI are not saved onto ClientScopesCondition.Configuration keycloak authorization-services * Password credential decoding from DB may fail in rare cases - No login possible keycloak * Dist.X cannot connect to external Postgres if the password ends with an = sign keycloak * Dist.X apparently doesn't apply correctly the db schema keycloak * JPA-Map storage might loose writes due to missing locking mechanism keycloak storage * Multiple active tabs when realm name equals name of tab in Admin console keycloak admin/ui * Missing german translation for webauthn-doAuthenticate keycloak translations * Hard coded message within account console v2 keycloak account/ui * Client Policies : Condition's negative logic configuration is not shown in Admin Console's form view keycloak * Placeholders in keycloak.properties do not get substituted at runtime after a build keycloak * Keycloak Server throws NPE at startup when the MAP_STORAGE feature is enabled keycloak storage * Setting "24 mins" to timeout, the admin console displays "1 day" keycloak admin/ui * Username editable when user is forced to re-authenticate keycloak authentication * Quarkus dist "providers" dir has outdated README keycloak dist/quarkus * Newline in localization messages causes uncaught syntax error in account console v2 keycloak account/ui * Dist.X argument parsing fails on semicolon keycloak dist/quarkus * KEYCLOAK-19289 check if values to set is not null keycloak * LDAP connection timeout is treated as login failure and brute force locking the user keycloak * Different method getGroupsCountByNameContaining in MapGroupProvider and JpaRealmProvider keycloak storage * MapRoleProvider could return also client roles when searching for realm roles keycloak storage * Missing DB constraints for JPA Map Storage for Clients keycloak storage * Scope bug in device authorization request keycloak * Handling lazy loading exceptions for storage in new and old storage keycloak storage * Model tests consistently time out keycloak storage * Keycloak.X cannot lookup embedded theme-resources from extension jars keycloak dist/quarkus * WebAuthnSigningInTest failures in pipeline keycloak testsuite * GHA failing due to wrong scheme when downloading ISPN server keycloak * Fixes for token revocation keycloak oidc * JPA Map storage doesn't downgrade entityVersion when modifying a row written with a future entityVersion keycloak storage * Updated flag disappearing for nested entities in HotRod store keycloak storage * Build command exits with success with invalid arguments keycloak dist/quarkus * Review guides to use the correct format for options keycloak docs * Mapped Quarkus properties should not be persisted keycloak dist/quarkus * Unstable model tests when starting multiple Infinispan instances keycloak storage * JPA Map storage doesn't increment version column on attribute update keycloak storage * Update Portuguese (Brazil) translations keycloak translations * Do not run re-augmentation if config is the same in dev mode keycloak dist/quarkus * Errors from CLI are masked by help keycloak dist/quarkus * Rename h2-file/h2-mem to dev-file/dev-mem and remove default values for username/password keycloak dist/quarkus * Keycloak is not capturing proper Signing details(like browser name and version) when logged in from different browsers keycloak account/ui * Not possible to register webauthn key on Firefox keycloak authentication/webauthn * JPA Map storage listener should handle optimistic locking for deleting entities keycloak storage * Failing to use cache remote-stores due to missing dependencies keycloak dist/quarkus * Can not set a jgroups stack other than the defaults from Infinispan keycloak dist/quarkus * JPA delegates can throw NoResultException when entity doesn't have any attributes keycloak storage UPGRADING Before you upgrade remember to backup your database and check the for anything that may have changed.</content><dc:creator /></entry></feed>
